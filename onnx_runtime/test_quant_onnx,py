import onnxruntime as ort
import numpy as np

print("Available providers:", ort.get_available_providers())

quantized_model = r"C:\Users\fenel\Documents\dcase2024_task1_baseline\baseline_static.onnx"

sess = ort.InferenceSession(quantized_model, providers=['CPUExecutionProvider'])
input_name = sess.get_inputs()[0].name

# Example input (adjust shape to your model input)
dummy_input = np.random.randn(1, 1, 256, 65).astype(np.float32)
outputs = sess.run(None, {input_name: dummy_input})
print("Inference output:", outputs)