Original model structure:
Network_1(
  (quant): QuantStub()
  (dequant): DeQuantStub()
  (in_c): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (stages): Sequential(
    (s1): Sequential(
      (b1): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (b2): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (b3): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential(
          (0): AvgPool2d(kernel_size=3, stride=(1, 2), padding=1)
          (1): Sequential()
        )
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
    )
    (s2): Sequential(
      (b4): Block(
        (after_block_activation): ReLU()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), groups=64, bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(64, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (b5): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(56, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(120, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
    )
    (s3): Sequential(
      (b6): Block(
        (after_block_activation): ReLU()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): Conv2d(56, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Conv2dNormActivation(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(120, 104, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
    )
  )
  (feed_forward): Sequential(
    (0): Conv2d(104, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): AdaptiveAvgPool2d(output_size=(1, 1))
  )
)
Before fusing, in_c module:
Sequential(
  (0): Conv2dNormActivation(
    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (1): Conv2dNormActivation(
    (0): Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
)
Fusing in_c module index 0 with keys: ['0', '1', '2']
Fusing in_c module index 1 with keys: ['0', '1', '2']
Before fusing, block b1:
Block(
  (after_block_activation): ReLU()
  (shortcut): Sequential()
  (block): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Conv2dNormActivation(
      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (ff): FloatFunctional(
    (activation_post_process): Identity()
  )
)
Fusing block b1 submodule 0 with keys: ['0', '1', '2']
Fusing block b1 submodule 1 with keys: ['0', '1', '2']
Fusing block b1 submodule 2 with keys: ['0', '1']
Before fusing, block b2:
Block(
  (after_block_activation): ReLU()
  (shortcut): Sequential()
  (block): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Conv2dNormActivation(
      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (ff): FloatFunctional(
    (activation_post_process): Identity()
  )
)
Fusing block b2 submodule 0 with keys: ['0', '1', '2']
Fusing block b2 submodule 1 with keys: ['0', '1', '2']
Fusing block b2 submodule 2 with keys: ['0', '1']
Before fusing, block b3:
Block(
  (after_block_activation): ReLU()
  (shortcut): Sequential(
    (0): AvgPool2d(kernel_size=3, stride=(1, 2), padding=1)
    (1): Sequential()
  )
  (block): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1), groups=64, bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Conv2dNormActivation(
      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (ff): FloatFunctional(
    (activation_post_process): Identity()
  )
)
Fusing block b3 submodule 0 with keys: ['0', '1', '2']
Fusing block b3 submodule 1 with keys: ['0', '1', '2']
Fusing block b3 submodule 2 with keys: ['0', '1']
Before fusing, block b4:
Block(
  (after_block_activation): ReLU()
  (block): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), groups=64, bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Conv2dNormActivation(
      (0): Conv2d(64, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (ff): FloatFunctional(
    (activation_post_process): Identity()
  )
)
Fusing block b4 submodule 0 with keys: ['0', '1', '2']
Fusing block b4 submodule 1 with keys: ['0', '1', '2']
Fusing block b4 submodule 2 with keys: ['0', '1']
Before fusing, block b5:
Block(
  (after_block_activation): ReLU()
  (shortcut): Sequential()
  (block): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(56, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Conv2dNormActivation(
      (0): Conv2d(120, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (ff): FloatFunctional(
    (activation_post_process): Identity()
  )
)
Fusing block b5 submodule 0 with keys: ['0', '1', '2']
Fusing block b5 submodule 1 with keys: ['0', '1', '2']
Fusing block b5 submodule 2 with keys: ['0', '1']
Before fusing, block b6:
Block(
  (after_block_activation): ReLU()
  (block): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(56, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (1): Conv2dNormActivation(
      (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (2): Conv2dNormActivation(
      (0): Conv2d(120, 104, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (ff): FloatFunctional(
    (activation_post_process): Identity()
  )
)
Fusing block b6 submodule 0 with keys: ['0', '1', '2']
Fusing block b6 submodule 1 with keys: ['0', '1', '2']
Fusing block b6 submodule 2 with keys: ['0', '1']
After fusing, model structure:
Network_1(
  (quant): QuantStub()
  (dequant): DeQuantStub()
  (in_c): Sequential(
    (0): Conv2dNormActivation(
      (0): ConvReLU2d(
        (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU()
      )
      (1): Identity()
      (2): Identity()
    )
    (1): Conv2dNormActivation(
      (0): ConvReLU2d(
        (0): Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU()
      )
      (1): Identity()
      (2): Identity()
    )
  )
  (stages): Sequential(
    (s1): Sequential(
      (b1): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): Identity()
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (b2): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): Identity()
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (b3): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential(
          (0): AvgPool2d(kernel_size=3, stride=(1, 2), padding=1)
          (1): Sequential()
        )
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1), groups=64)
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): Identity()
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
    )
    (s2): Sequential(
      (b4): Block(
        (after_block_activation): ReLU()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), groups=64)
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(64, 56, kernel_size=(1, 1), stride=(1, 1))
            (1): Identity()
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
      (b5): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(56, 120, kernel_size=(1, 1), stride=(1, 1))
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120)
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(120, 56, kernel_size=(1, 1), stride=(1, 1))
            (1): Identity()
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
    )
    (s3): Sequential(
      (b6): Block(
        (after_block_activation): ReLU()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(56, 120, kernel_size=(1, 1), stride=(1, 1))
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): ConvReLU2d(
              (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120)
              (1): ReLU()
            )
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): Conv2d(120, 104, kernel_size=(1, 1), stride=(1, 1))
            (1): Identity()
          )
        )
        (ff): FloatFunctional(
          (activation_post_process): Identity()
        )
      )
    )
  )
  (feed_forward): Sequential(
    (0): Conv2d(104, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): AdaptiveAvgPool2d(output_size=(1, 1))
  )
)
Quantized model:8
Network_1(
  (quant): Quantize(scale=tensor([0.1746]), zero_point=tensor([65]), dtype=torch.quint8)
  (dequant): DeQuantize()
  (in_c): Sequential(
    (0): Conv2dNormActivation(
      (0): QuantizedConvReLU2d(1, 8, kernel_size=(3, 3), stride=(2, 2), scale=0.059376418590545654, zero_point=0, padding=(1, 1))
      (1): Identity()
      (2): Identity()
    )
    (1): Conv2dNormActivation(
      (0): QuantizedConvReLU2d(8, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.05638470873236656, zero_point=0, padding=(1, 1))
      (1): Identity()
      (2): Identity()
    )
  )
  (stages): Sequential(
    (s1): Sequential(
      (b1): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(32, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.077146977186203, zero_point=0)
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.09372740238904953, zero_point=0, padding=(1, 1), groups=64)
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): QuantizedConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.13327153027057648, zero_point=60)
            (1): Identity()
          )
        )
        (ff): QFunctional(
          scale=0.15220390260219574, zero_point=51
          (activation_post_process): Identity()
        )
      )
      (b2): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(32, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.08058259636163712, zero_point=0)
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.09682220220565796, zero_point=0, padding=(1, 1), groups=64)
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): QuantizedConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.17268182337284088, zero_point=66)
            (1): Identity()
          )
        )
        (ff): QFunctional(
          scale=0.18891578912734985, zero_point=54
          (activation_post_process): Identity()
        )
      )
      (b3): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential(
          (0): AvgPool2d(kernel_size=3, stride=(1, 2), padding=1)
          (1): Sequential()
        )
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(32, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.07541500777006149, zero_point=0)
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 2), scale=0.09542886912822723, zero_point=0, padding=(1, 1), groups=64)
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): QuantizedConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.12231174856424332, zero_point=59)
            (1): Identity()
          )
        )
        (ff): QFunctional(
          scale=0.17039820551872253, zero_point=41
          (activation_post_process): Identity()
        )
      )
    )
    (s2): Sequential(
      (b4): Block(
        (after_block_activation): ReLU()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(32, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.08333487808704376, zero_point=0)
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(2, 1), scale=0.095490962266922, zero_point=0, padding=(1, 1), groups=64)
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): QuantizedConv2d(64, 56, kernel_size=(1, 1), stride=(1, 1), scale=0.1267545521259308, zero_point=73)
            (1): Identity()
          )
        )
        (ff): QFunctional(
          scale=1.0, zero_point=0
          (activation_post_process): Identity()
        )
      )
      (b5): Block(
        (after_block_activation): ReLU()
        (shortcut): Sequential()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(56, 120, kernel_size=(1, 1), stride=(1, 1), scale=0.08105675131082535, zero_point=0)
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(120, 120, kernel_size=(3, 3), stride=(1, 1), scale=0.08963710069656372, zero_point=0, padding=(1, 1), groups=120)
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): QuantizedConv2d(120, 56, kernel_size=(1, 1), stride=(1, 1), scale=0.14293217658996582, zero_point=68)
            (1): Identity()
          )
        )
        (ff): QFunctional(
          scale=0.1449466347694397, zero_point=63
          (activation_post_process): Identity()
        )
      )
    )
    (s3): Sequential(
      (b6): Block(
        (after_block_activation): ReLU()
        (block): Sequential(
          (0): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(56, 120, kernel_size=(1, 1), stride=(1, 1), scale=0.06929101794958115, zero_point=0)
            (1): Identity()
            (2): Identity()
          )
          (1): Conv2dNormActivation(
            (0): QuantizedConvReLU2d(120, 120, kernel_size=(3, 3), stride=(1, 1), scale=0.11869516223669052, zero_point=0, padding=(1, 1), groups=120)
            (1): Identity()
            (2): Identity()
          )
          (2): Conv2dNormActivation(
            (0): QuantizedConv2d(120, 104, kernel_size=(1, 1), stride=(1, 1), scale=0.13730275630950928, zero_point=77)
            (1): Identity()
          )
        )
        (ff): QFunctional(
          scale=1.0, zero_point=0
          (activation_post_process): Identity()
        )
      )
    )
  )
  (feed_forward): Sequential(
    (0): QuantizedConv2d(104, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.3472730219364166, zero_point=42, bias=False)
    (1): QuantizedBatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): AdaptiveAvgPool2d(output_size=(1, 1))
  )
)
























































































































































































































Batch: 116/116 -- Batch Accuracy: 104/240
Quantized model accuracy: 40.64%
Unquantized model accuracy: 41.00%
tensor([[-3.8160, -0.6360, -3.1800,  2.5440,  6.3600,  0.0000, -3.8160, -3.8160,
          6.3600, -3.1800]])